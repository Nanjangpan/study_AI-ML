{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 스터디 1주차\n",
    "#### 2020.01.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamentals of Machine Learning\n",
    "* Learn the most classical methods of machine learning\n",
    "    * Rule based approach\n",
    "    * Classical statistics approach\n",
    "    * Information theory appraoch\n",
    "<br><br>\n",
    "* Rule based machine learning\n",
    "    * How to find the specialized and the generalized rules\n",
    "    * Why the rules are easily broken\n",
    "<br><br>\n",
    "* Decision Tree\n",
    "    * How to create a decision tree given a training dataset\n",
    "    * Why the tree becomes a weak learner with a new dataset\n",
    "<br><br>\n",
    "* Linear Regression\n",
    "    * How to infer a parameter set from a training dataset\n",
    "    * Why the feature engineering has its limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Rule based Machine Learning\n",
    "### A Perfect World\n",
    "* No observation errors, No inconsistent observations\n",
    "* No stochastic elements in the system we observe\n",
    "* Full information in the observations to regenerate the system\n",
    "\n",
    "### Rule based Machine Learning\n",
    "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decision Tree\n",
    "### Entropy\n",
    "**확률이 낮을수록, 어떤 정보일지는 불확실하게 되고, 우리는 이때 ‘정보가 많다’, ‘엔트로피가 높다’고 표현한다.**\n",
    "<br><br>\n",
    "정보 이론의 기본은, 어떤 사람이 정보를 더 많이 알수록 새롭게 알 수 있는 정보는 적어진다는 것이다. 어떤 사건의 확률이 매우 높다고 가정하자. 우리는 그 사건이 발생해도 별로 놀라지 않는다. 즉, 이 사건은 적은 정보를 제공한다. 반대로, 만약 사건이 불확실하다면, 그 사건이 일어났을 때 훨씬 유용한 정보를 제공한다. **그러므로, 정보량(information content)은 확률에 반비례한다.** 이제 만약 더 많은 사건이 일어난다면, 엔트로피는 실제로 한 사건이 일어났을 때, 얻을 것으로 기대되는 평균 정보량을 측정한다. 주사위 던지기와 동전 던지기를 생각해보자. 주사위 던지기에서 일어나는 한 사건의 확률은 동전 던지기에서 일어나는 한 사건의 확률보다 작다. 즉, 여기서 엔트로피는 주사위 던지기가 동전 던지기보다 크다고 할 수 있다.\n",
    "<br><br>\n",
    "그러므로 엔트로피는 '**어떤 상태에서의 불확실성** , 또는 이와 동등한 의미로 ‘평균 정보량’을 의미한다. 이 용어를 직관적으로 이해하기 위해 정치에서의 선거를 생각해보자. 보통, 그러한 선거는 우리가 선거 결과를 모르기 때문에 실시한다. 즉, 선거 결과는 상대적으로 ‘불확실’하다. 그리고 실제로 선거를 시행하고 선거 결과를 얻는 것은 우리에게 ‘새로운 정보’를 제공한다. 이 말은 선거 결과에 대한 엔트로피가 크다는 말과 같다. 이제, 첫 번째 선거가 실시되고 난 후, 두 번재 선거가 실시되었다고 생각해보자. 첫 번째 선겨 결과를 이미 알기 때문에, 우리는 두 번째 선거 결과를 예측할 수 있고 두 번째 선거 결과는 우리에게 더 이상 우리에게 아무런 정보를 제공하지 않는다. 이 경우, 우리는 두 번째 선거 결과의 엔트로피가 첫 번째 것보다 작다.\n",
    "<br><br>\n",
    "엔트로피는 다음과 같은 수식으로 표현 가능하다.\n",
    "<br><br>\n",
    "$$H(X)=-\\Sigma_xP(X=x)log_bP(X=x)$$\n",
    "<br><br>\n",
    "조건부 엔트로피의 식은 다음과 같다.\n",
    "<br><br>\n",
    "$$\n",
    "H(Y|X)=\\Sigma_xP(X=x)H(Y|X=x)\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "H(Y|X)=\\Sigma_xP(X=x)(-\\Sigma_xP(Y=y|X=x)log_bP(Y=y|X=x))\n",
    "$$\n",
    "<br><br>\n",
    "\n",
    "### Information Gain\n",
    "우리는 정보의 불확실성을 최소화하는 방법을 추구한다. 이를 위해 **Information Gain**을 정의할 수 있다. 식은 다음과 같다.\n",
    "<br><br>\n",
    "$$\n",
    "IG(Y,A_i) = H(Y)-H(Y|A_i)\n",
    "$$\n",
    "<br><br>\n",
    "Information Gain은 **전체 엔트로피에서 조건부 엔트로피를 뺀 값**이다. 조건부 엔트로피는 $A_i$라는 선택을 한 이후의 엔트로피이다. 당연히 조건부 엔트로피는 전체 엔트로피보다 작고, 전체 엔트로피에서 조건부 엔트로피를 뺀 Information Gain은 $A_i$**라는 선택을 함으로써 얻게 된 정보량의 이득**으로 해석할 수 있다.<br>\n",
    "**즉, 우리는 Information Gain이 가장 큰 선택의 기준을 Decision Tree의 첫번째 노드로 사용할 수 있는 것이다.** 엔트로피를 통해 계산된 Information Gain을 기준으로 Tree의 노드를 정하고 더 이상의 구분이 의미가 없어질 때까지 가지를 뻗을 수 있다.\n",
    "\n",
    "### Problem of Decision Tree\n",
    "훈련데이터에 대해 무제한으로 가지를 뻗어나가며 학습을 진행하면 **과적합(overfitting)** 의 위험이 크다. 훈련데이터는 실제의 완벽한 표본이 아니기 때문에 언제나 Cross-Validation 등의 평가방법을 통해 과적합의 위험을 방지할 필요가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear Regression\n",
    "### Assumptions of Simple Linear Regression\n",
    "<br><br>\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "<br><br>\n",
    "단순회귀분석의 기본회귀식은 위와 같다. 회귀분석은 직관적으로 데이터를 표현하기에 좋은 추정방법이지만, 많은 가정사항이 필요하기 때문에 분석에 유의해야 한다.\n",
    "\n",
    "* Linearity of model\n",
    "* Normality of error\n",
    "* Equal Variance of error\n",
    "* Independence of error\n",
    "\n",
    "### Estimation of Coefficients(Least Square Method)\n",
    "<br><br>\n",
    "$$\n",
    "Q(\\beta_0, \\beta_1) = \\Sigma(y-\\hat{y})^2\n",
    "$$\n",
    "<br><br>\n",
    "$$\n",
    "(\\hat{\\beta_0}, \\hat{\\beta_1}) = argmin_{\\beta_0, \\beta_1} Q(\\beta_0, \\beta_1) \n",
    "$$\n",
    "<br><br>\n",
    "회귀계수의 추정은 최소자승법(Least Square Method)를 통해 계산된다. SSE를 최소화하는 기준으로 $\\hat{\\beta_0}, \\hat{\\beta_1}$를 추정한다. LSM으로 추정된 회귀계수는 Gauss-Markov Theorem에 의해 **Best Unbiased Linear Estimator**임이 증명되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
