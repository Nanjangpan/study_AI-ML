{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 5. Support Vector Machine\n",
    "\n",
    "**복잡한 분류문제, 작거나 중간 크기의 데이터셋**\n",
    "\n",
    "* Decision Boundary with Margin\n",
    "* Maximizing the Margin\n",
    "* Error Handling in SVM-> Soft Margin with SVM\n",
    "* Kernel Trick...(Primal and Dual problem)\n",
    "* SVM with Kernel\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Boundary with Margin\n",
    "\n",
    "주어진 샘플의 종류를 잘 구분할 수 있는 경계\n",
    "\n",
    "어떤 하나의 값이 아님. 고차원에서 샘플을 분류해내는 경계도 됨. \n",
    "\n",
    "![](images/캡처.PNG)\n",
    "\n",
    "\n",
    "\n",
    "확률적인 방법(나이브 베이즈 등)을 제외하고 decision boundary를 정한다면 how? \n",
    "\n",
    "---\n",
    "\n",
    "![](images/image-20200205012835099.png)\n",
    "\n",
    "* Support Vector Machine\n",
    "\n",
    "![image-20200205013152216](images/image-20200205013152216.png)\n",
    "\n",
    "### Decision boundary line\n",
    "\n",
    "$$\n",
    "wx + b  = 0\n",
    "$$\n",
    '\n",
    "$$\n",
    "(x_1, x_2, b)\n",
    "$$\n",
    "\n",
    "![image-20200205020453505](images/image-20200205020453505.png)\n",
    "\n",
    "\n",
    "$$\n",
    "positive \\; case : wx + b > 0 \\\\\n",
    "negative \\; case : wx + b < 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "confidence\\; level\\\\\n",
    "positive\\;case : (wx+b)y =>\\; ++\\\\\n",
    "negative\\;case : (wx+b)y =>\\; --\\\\\n",
    "\\;\\\\\n",
    "confidence\\;level >0\n",
    "$$\n",
    "\n",
    "confidence level을 최대한 높이는 w와 b를 찾아야 함.\n",
    "\n",
    "**Margin**\n",
    "\n",
    "![image-20200205022535036](images/image-20200205022535036.png)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Maximizing the Margin\n",
    "\n",
    "![image-20200205022936907](images/image-20200205022936907.png)\n",
    "$$\n",
    "f(x) = wx + b\\\\\n",
    "point \\;x_p \\;->\n",
    "f(x) = wx+b = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "positive\\;point\\;x\\\\\n",
    "f(x) = wx + b = a, a>0\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "distance\\\\\n",
    "x = x_p + r\\frac{w}{||w||}, f(x_p) = 0\\\\\n",
    "$$\n",
    "\n",
    "![image-20200205025533668](images/image-20200205025533668.png)\n",
    "\n",
    "위의 x를 f(x) 안에 집어 넣으면...\n",
    "$$\n",
    "f(x) = w·x + b = w(x_p + r\\frac{w}{||w||}) +b = wx_p + b+r\\frac{w·w}{||w||} = r||w||\\\\\n",
    "*wx_p+b = 0\\\\\n",
    "\\,\\\\\n",
    "f(x) = r||w|| =a\n",
    "\\,\\\\\n",
    "\\,\\\\\n",
    "distance\\; r = \\frac{f(x)}{||w||} = \\frac{a}{||w||}\n",
    "$$\n",
    "\n",
    "\n",
    "결국 good decision boundary를 위해서는 w가 중요\n",
    "\n",
    "![image-20200205030722903](images/image-20200205030722903.png)\n",
    "\n",
    "### 2r의 최적화 문제\n",
    "\n",
    "$$\n",
    "max_{x,b}2r = \\frac{2a}{||w||}\\\\\n",
    "조건은\\, (wx_j+b)y_j \\geq a, j=instance(data)\n",
    "\\,\\\\\n",
    "a는\\, arbitrary\\;number이므로\\; 1로\\;정해도\\;됨\\\\\n",
    "max_{x,b}2r = \\frac{2}{||w||}이\\;됨.\\\\\n",
    "\\,\\\\\n",
    "…\\\\\n",
    "\\;\\\\\n",
    "minimize\\;w의\\;문제로 \\;변환\\\\\n",
    "min_{w,b}||w||, 조건\\; (wx_j+b)y_j \\geq 1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**||w||가 quadratic optimization문제가 되는 이유**\n",
    "$$\n",
    "||w|| \\\\\n",
    "w=> w_1, w_2\\\\\n",
    "\\;\\\\\n",
    "||w|| = \\sqrt{w_1^2+w_2^2}\\\\\n",
    "square\\;problem\n",
    "$$\n",
    "\n",
    "*good quadsolution\n",
    "\n",
    "![image-20200205034033529](images/image-20200205034033529.png)\n",
    "\n",
    "\n",
    "\n",
    "*bad quadsolution\n",
    "\n",
    "![image-20200205034334068](images/image-20200205034334068.png)\n",
    "\n",
    "![image-20200205034147501](images/image-20200205034147501.png)\n",
    "\n",
    "왜 bad = w를 찾지 못하냐 => SVM with Hard Margin으로 error를 인정하지 않기 때문.\n",
    "\n",
    "error를 어느 정도 인정하면 SVM with Soft Margin\n",
    "\n",
    "* SVM Hard Margin\n",
    "* SVM Soft Margin\n",
    "* SVM with Kernel(Hard Margin)\n",
    "\n",
    "---\n",
    "\n",
    "# Error Handling in SVM\n",
    "\n",
    "![image-20200205035057702](images/image-20200205035057702.png)\n",
    "\n",
    "### Error handling\n",
    "\n",
    "1. **complex decision boundary(->Kernel Trick)**\n",
    "\n",
    "2. error penalization(-> Soft Margin SVM)\n",
    "\n",
    "\n",
    "\n",
    "### Error penalization\n",
    "\n",
    "1. counting errors\n",
    "   $$\n",
    "   min_{w,b}||w|| + C*N_{error}\n",
    "   $$\n",
    "   ![image-20200205040501720](images/image-20200205040501720.png)\n",
    "\n",
    "0-1 Loss : quadratic problem으로 정의 어려움, decision boundary 너머의 점들에 대해 똑같은 penalty 부여\n",
    "\n",
    "\n",
    "\n",
    "**Hinge Loss**\n",
    "\n",
    "![image-20200205040906408](images/image-20200205040906408.png)\n",
    "$$\n",
    "\\zeta_j = slack \\;variable, >1\\; when\\; mis-classified\n",
    "\\;\\\\\n",
    "min_{w,b}||w|| + C\\sum_j\\zeta_j\\\\\n",
    "조건\\; (wx_j+b)y_j \\geq 1-\\zeta_j\\;,일부\\;케이스에서는\\;\\zeta값이\\;있으므로.기본적으로\\;\\zeta\\geq0\\\\\n",
    "hard\\;margin\\;SVM : (wx_j+b)y_j \\geq 1\\\\\n",
    "$$\n",
    "C: slack variable을 어느 정도의 강도로 설명할 것인지, trade-off parameter\n",
    "\n",
    "summation은 minimum\n",
    "\n",
    "---\n",
    "\n",
    "# Soft Margin with SVM\n",
    "\n",
    "![image-20200205042723057](images/image-20200205042723057.png)\n",
    "$$\n",
    "min_{w,b}||w|| + C\\sum_j\\zeta_j\\\\\n",
    "조건\\; (wx_j+b)y_j \\geq 1-\\zeta_j\n",
    "$$\n",
    "점이 decision boundary를 넘어가도 되는데 넘어갈 때마다 penalize된다=soft margin SVM\n",
    "\n",
    "* Log-loss, hinge loss, 0-1 loss\n",
    "\n",
    "  ![image-20200205043404600](images/image-20200205043404600.png)\n",
    "\n",
    "  -log loss는 안전한 사이드에 있어도 penalty를 줌. \n",
    "\n",
    "  -hinge는 안전한 사이드를 완전하게 신뢰\n",
    "\n",
    "  \n",
    "\n",
    "**C값이 변화하면서 바뀌는 decision boundary**\n",
    "\n",
    "![image-20200205043817592](images/image-20200205043817592.png)\n",
    "\n",
    "-무조건 C가 크면 좋다? No. C=10, C=10000 큰 차이 없음. ==> 결국 실제 적용은 EDA 부분에서 조절해줘야 함.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Kernel Trick\n",
    "\n",
    "-non linear가 일관된 trend가 있다고 하면.. 즉, 데이터가 complex하다면..\n",
    "\n",
    "\n",
    "\n",
    "### Error handling\n",
    "\n",
    "1. complex decision boundary(->Kernel Trick)\n",
    "\n",
    "2. **error penalization(-> Soft Margin SVM)**\n",
    "\n",
    "\n",
    "\n",
    "![image-20200205044303308](images/image-20200205044303308.png)\n",
    "\n",
    "![image-20200205045018659](images/image-20200205045018659.png)\n",
    "\n",
    "-linear regression(higher dimension) : interaction terms\n",
    "$$\n",
    "X_1, X_2, X_1^2, X_2^2, X_1X_2, X_1^3, X_2^3...\n",
    "$$\n",
    "-higher dimension with SVM? \n",
    "\n",
    "Primal problem-> Dual problem\n",
    "\n",
    "![image-20200205045400242](images/image-20200205045400242.png)\n",
    "\n",
    "SVM\n",
    "\n",
    ": Classification-> Constrained quadratic programming, Constrained optimization\n",
    "\n",
    "\n",
    "\n",
    "**라그랑주 승수법**\n",
    "\n",
    "(강의랑 조금 다름)\n",
    "\n",
    "-제약이 있는 최적화 문제를 풀 때 사용, 람다는 나중에 없어지는 변수\n",
    "$$\n",
    "제약조건: g(x,y)\\\\\n",
    "최적화 함수: f(x,y)\\\\\n",
    "\\;\\\\\n",
    "1. g(x,y) = 0일 때 \\lambda g(x,y) = 0\\\\\n",
    "2. L = f(x,y) - \\lambda g(x,y)일 때 L(x,y,\\lambda)의\\;최댓값과\\;최솟값은 \\;g(x,y)=0일때\\\\\n",
    "\\; f(x,y)의\\;최댓값과\\;최솟값을\\;의미함\\\\\n",
    "3. \\frac{\\partial L}{\\partial x}= 0, \\frac{\\partial L}{\\partial y}= 0, \\frac{\\partial L}{\\partial \\lambda}= 0일때\\; L은\\;최댓값이나\\;최솟값(극값)을\\;가짐.\\\\\n",
    "4. 정리=> \\nabla L = (\\frac{\\partial L}{\\partial x}, \\frac{\\partial L}{\\partial y}, \\frac{\\partial L}{\\partial \\lambda}) = \\lim 0\n",
    "$$\n",
    "![image-20200205051548128](images/image-20200205051548128.png)\n",
    "\n",
    "예시)\n",
    "\n",
    "![image-20200205051644452](images/image-20200205051644452.png)\n",
    "\n",
    "유도방법: https://untitledtblog.tistory.com/96\n",
    "\n",
    "라그랑주 설명(외국사이트): http://www.slimy.com/~steuard/teaching/tutorials/Lagrange.html\n",
    "\n",
    "\n",
    "\n",
    "강의\n",
    "$$\n",
    "constrained \\; optimization: min_xf(x)\\\\\n",
    "condition: g(x) \\leq 0, h(x) = 0 \\\\\n",
    "Lagrange\\;Prime\\;Function: L(x,\\alpha, \\beta) = f(x) + \\alpha g(x) + \\beta h(x)\\\\\n",
    "Lagrange\\;Multiplier: \\alpha \\geq 0, \\beta\\\\\n",
    "Lagrange\\;Dual\\;Function: d(\\alpha, \\beta) = inf_{x \\in X}L(x,\\alpha,\\beta)=min_xL(x,\\alpha, \\beta)\n",
    "$$\n",
    "![image-20200205052930437](images/image-20200205052930437.png)\n",
    "\n",
    "\n",
    "\n",
    "->primal problem을 dual problem으로 풀겠다가 핵심\n",
    "\n",
    "![image-20200205053857866](images/image-20200205053857866.png)\n",
    "\n",
    "![image-20200205054036126](images/image-20200205054036126.png)\n",
    "\n",
    "Dual Problem의 속성\n",
    "\n",
    "-Weak duality theorem: 최대화 문제면 primal problem의 최적해에 대한 상한이 dual problem의 값이다.\n",
    "\n",
    "-Strong duality: dual problem에서 최소 상한값이 primal problem에서의 최적값과 같다. 이때 KKT조건이 만족되어야 함.\n",
    "\n",
    "\n",
    "\n",
    "-KKT\n",
    "\n",
    "![image-20200205054807294](images/image-20200205054807294.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-20200205055146243](images/image-20200205055146243.png)\n",
    "\n",
    "![image-20200205055213252](images/image-20200205055213252.png)\n",
    "\n",
    "![image-20200205055229040](images/image-20200205055229040.png)\n",
    "\n",
    "\n",
    "\n",
    "KKT 만족하는지 w,b에 대해 미분해서 확인\n",
    "\n",
    "![image-20200205055551765](images/image-20200205055551765.png)\n",
    "\n",
    "![image-20200205055914441](images/image-20200205055914441.png)\n",
    "$$\n",
    "\\alpha의 \\;2차식으로 \\;돌아옴=quadratic\\;programming\n",
    "$$\n",
    "![image-20200205060055089](images/image-20200205060055089.png)\n",
    "\n",
    "\n",
    "\n",
    "**Mapping Function**\n",
    "\n",
    "linearly unseparable-> separable\n",
    "\n",
    "![image-20200205060402756](images/image-20200205060402756.png)\n",
    "\n",
    "편리, but too much interaction(dimensions)----> Kernel solution\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Kernel Function\n",
    "\n",
    "![image-20200205060650814](images/image-20200205060650814.png)\n",
    "\n",
    "K=내적, 각자 다른 dimension\n",
    "\n",
    "\n",
    "\n",
    "커널 종류\n",
    "\n",
    "-선형\n",
    "\n",
    "-다항식\n",
    "\n",
    "-가우시안\n",
    "\n",
    "-시그모이드\n",
    "\n",
    "\n",
    "\n",
    "x와 z를 먼저 내적해서 3차로 하는 것(Kernel trick, 연산량 작음)= x와 z를 다른 차원으로 보내서 그걸 내적하는 것(Kernel, 연산량 폭증)\n",
    "\n",
    "\n",
    "\n",
    "![image-20200205061838017](images/image-20200205061838017.png)\n",
    "\n",
    "![image-20200205061914156](images/image-20200205061914156.png)\n",
    "\n",
    "\n",
    "\n",
    "**SVM Kernel trick**\n",
    "\n",
    "![image-20200205062517280](images/image-20200205062517280.png)\n",
    "\n",
    "\n",
    "\n",
    "<정리>\n",
    "\n",
    "Hard Margin(오류)--> Soft Margin(정확하지 X)-->Kernel(연산량 폭증)--> Kernel Trick\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "# Ch 6. Evaluation(Train/Test, Regularization)\n",
    "\n",
    "\n",
    "\n",
    "* 온라인 학습\n",
    "* Error 값\n",
    "* CV\n",
    "* Regularization\n",
    "\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "### 온라인 학습\n",
    "\n",
    ": 데이터를 순차적으로 한 개씩 또는 **미니배치**mini-batch 라 부르는 작은 묶음 단위로 주입하여 시스템을 훈련시킵니다. 매 학습 단계가 빠르고 비용이 적게 들어 시스템은 데이터가 도착하는 대로 즉시 학습할 수 있습니다(그림 1 -13).연속적으로 데이터를 받고(예를 들면 주식가격) 빠른 변화에 스스로 적응해야 하는 시스템에 적합합니다. 컴퓨팅 자원이 제한된 경우에도 좋은 선택입니다. 온라인 학습 시스템이 새로운 데이터 샘플을 학습하면 학습이 끝난 데이터는 더 이상 필요하지 않으므로 버리면 됩니다(이전 상태로 되돌릴 수 있도록 데이터를 재사용하기 위해 보관할 필요가 없다면). 이렇게 되면 많은 공간을 절약할 수 있습니다.(핸즈온머신러닝, 2018)\n",
    "\n",
    "![스크린샷 2018-05-24 오후 6.20.09](https://tensorflowkorea.files.wordpress.com/2018/05/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2018-05-24-e1848be185a9e18492e185ae-6-20-09.png?w=625)\n",
    "\n",
    "![스크린샷 2018-05-24 오후 6.23.50](https://tensorflowkorea.files.wordpress.com/2018/05/e18489e185b3e1848fe185b3e18485e185b5e186abe18489e185a3e186ba-2018-05-24-e1848be185a9e18492e185ae-6-23-50.png?w=625)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Error값\n",
    "\n",
    "![image-20200205064739576](images/image-20200205064739576.png)\n",
    "\n",
    "• f: the target function to learn\n",
    "\n",
    "• g: the learning function of ML \n",
    "\n",
    "• $g^(D)$: the learned function by using a dataset, D, or an instance of hypothesis \n",
    "\n",
    "• D: an available dataset drawn from the real world\n",
    "\n",
    " • $\\overline g$: the average hypothesis of a given infinite number of Ds\n",
    "\n",
    "\n",
    "\n",
    "![image-20200205071809892](images/image-20200205071809892.png)\n",
    "\n",
    "variance-bias : trade off relation\n",
    "\n",
    "bias: 모델을 더 복잡하게. fitting-> overfitting\n",
    "\n",
    "\n",
    "\n",
    "안전하고 정확도 낮은 모델 vs 위험하고 일부 정확도 높은 모델=> trade off\n",
    "\n",
    "![image-20200205073816185](images/image-20200205073816185.png)\n",
    "\n",
    "complex model(right) : high variance\n",
    "\n",
    "simple model(left) : high bias\n",
    "\n",
    "\n",
    "\n",
    "오컴의 면도날: 동일한 모델이면 low complexity를 가진 것으로 선택\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    ": mimic infinite number of sampling\n",
    "\n",
    "![image-20200205074331534](images/image-20200205074331534.png)\n",
    "\n",
    "![image-20200205074358601](images/image-20200205074358601.png)\n",
    "\n",
    "LOOCV: 하나 남겨두고 모두 테스팅 * N개(Leave One Out Cross Validation)\n",
    "\n",
    "\n",
    "\n",
    "* Precision/Recall/F1 measure\n",
    "\n",
    "![image-20200205074608671](images/image-20200205074608671.png)\n",
    "\n",
    "Precision = $\\frac{TP} {(TP+FP)}$ = 예측한 것 중에 얼마나 맞췄는지\n",
    "\n",
    "Recall = $\\frac {TP} { (TP+FN)}$ = 답 중에 얼마나 맞췄는지\n",
    "\n",
    "F1 -Measure= $2 \\frac {(Precision * Recall)} {(Precision + Recall)}$\n",
    "\n",
    "\n",
    "\n",
    "* Fb -Measure = $(1+b^2 ) \\frac {(Precision * Recall)} {(b^2 Precision + Recall)}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization\n",
    "\n",
    ": perfect fit을 포기함. test set의 potential을 높이기 위함.\n",
    "\n",
    "\n",
    "\n",
    "E(w) = error term\n",
    "\n",
    "![image-20200205080105587](images/image-20200205080105587.png)\n",
    "\n",
    "![image-20200205080241959](images/image-20200205080241959.png)\n",
    "\n",
    "Ridge가 더 보편적(파라미터가 0이 되진 않음). 다 반영할 수 있음. \n",
    "\n",
    "Lasso는 많은 파라미터가 날라감(0이 됨). 몇몇 features 만 반응. 반응 빠름.\n",
    "\n",
    "\n",
    "\n",
    "Regularization effect\n",
    "\n",
    "![image-20200205080928188](images/image-20200205080928188.png)\n",
    "\n",
    "\n",
    "\n",
    "![image-20200205081102574](images/image-20200205081102574.png)\n",
    "\n",
    "\n",
    "\n",
    "SVM Regularization => C parameter\n",
    "\n",
    "![image-20200205081320996](images/image-20200205081320996.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
